{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "from glob import glob\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from skimage.color import rgb2gray\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model, load_model, save_model\n",
    "from tensorflow.keras.layers import Input, Activation, BatchNormalization, Dropout, Lambda, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'vision' already exists and is not an empty directory.\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "'cp' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'cp' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'cp' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'cp' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'cp' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cython in c:\\users\\prayag chawla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.29.35)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: \"'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\": Expected package name at the start of dependency specifier\n",
      "    'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
      "    ^\n",
      "Hint: = is not a valid operator. Did you mean == ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations in c:\\users\\prayag chawla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.4.11)\n",
      "Requirement already satisfied: numpy>=1.24.4 in c:\\users\\prayag chawla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from albumentations) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\prayag chawla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from albumentations) (1.10.0)\n",
      "Requirement already satisfied: scikit-image>=0.21.0 in c:\\users\\prayag chawla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from albumentations) (0.24.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\prayag chawla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from albumentations) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\prayag chawla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from albumentations) (4.10.0)\n",
      "Requirement already satisfied: scikit-learn>=1.3.2 in c:\\users\\prayag chawla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from albumentations) (1.5.1)\n",
      "Requirement already satisfied: pydantic>=2.7.0 in c:\\users\\prayag chawla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from albumentations) (2.7.4)\n",
      "Requirement already satisfied: albucore>=0.0.11 in c:\\users\\prayag chawla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from albumentations) (0.0.12)\n",
      "Requirement already satisfied: eval-type-backport in c:\\users\\prayag chawla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from albumentations) (0.2.0)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in c:\\users\\prayag chawla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from albumentations) (4.10.0.84)\n",
      "Requirement already satisfied: tomli>=2.0.1 in c:\\users\\prayag chawla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from albucore>=0.0.11->albumentations) (2.0.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\prayag chawla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in c:\\users\\prayag chawla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic>=2.7.0->albumentations) (2.18.4)\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\users\\prayag chawla\\appdata\\roaming\\python\\python311\\site-packages (from scikit-image>=0.21.0->albumentations) (3.1)\n",
      "Requirement already satisfied: pillow>=9.1 in c:\\users\\prayag chawla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-image>=0.21.0->albumentations) (10.4.0)\n",
      "Requirement already satisfied: imageio>=2.33 in c:\\users\\prayag chawla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-image>=0.21.0->albumentations) (2.34.2)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\prayag chawla\\appdata\\roaming\\python\\python311\\site-packages (from scikit-image>=0.21.0->albumentations) (2023.4.12)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\prayag chawla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-image>=0.21.0->albumentations) (23.2)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in c:\\users\\prayag chawla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-image>=0.21.0->albumentations) (0.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\prayag chawla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=1.3.2->albumentations) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\prayag chawla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=1.3.2->albumentations) (3.1.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\prayag chawla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\prayag chawla\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from opencv-python) (1.24.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The syntax of the command is incorrect.\n",
      "'cp' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'cp' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'cp' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'cp' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# references/detection\n",
    "!git clone https://github.com/pytorch/vision.git\n",
    "!cd vision\n",
    "!git checkout v0.8.2\n",
    "\n",
    "!cp ./vision/references/detection/utils.py ./\n",
    "!cp ./vision/references/detection/transforms.py ./\n",
    "!cp ./vision/references/detection/coco_eval.py ./\n",
    "!cp ./vision/references/detection/engine.py ./\n",
    "!cp ./vision/references/detection/coco_utils.py ./\n",
    "\n",
    "!pip install cython\n",
    "# Install pycocotools, the version by default in Colab\n",
    "!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
    "!pip install -U albumentations\n",
    "!pip install -U opencv-python\n",
    "\n",
    "#Copy and unify the train and validation datasets into one folder for images and another for labels\n",
    "!mkdir ./train\n",
    "!cp -a /kaggle/input/martianlunar-crater-detection-dataset/craters/train/images/. ./train/images/\n",
    "!cp -a /kaggle/input/martianlunar-crater-detection-dataset/craters/valid/images/. ./train/images/\n",
    "!cp -a /kaggle/input/martianlunar-crater-detection-dataset/craters/train/labels/. ./train/labels/\n",
    "!cp -a /kaggle/input/martianlunar-crater-detection-dataset/craters/valid/labels/. ./train/labels/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import utils\n",
    "# import transforms as T  # Use OpenCV or Pillow for image transformations instead\n",
    "import cv2\n",
    "import time\n",
    "# from albumentations.pytorch.transforms import ToTensorV2  # Use PyTorch or other framework-specific modules\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from scipy import stats  # Alternative for statistical functions\n",
    "from PIL import Image  # Alternative for basic image transformations\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CraterDataset(object):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(self.root, \"images\"))))\n",
    "        self.annots = list(sorted(os.listdir(os.path.join(self.root, \"labels\"))))\n",
    "        self.classes = ['Background','Crater']\n",
    "        \n",
    "    # Converts boundry box formats, this version assumes single class only!\n",
    "    def convert_box_cord(self,bboxs, format_from, format_to, img_shape):\n",
    "        if format_from == 'normxywh':\n",
    "            if format_to == 'xyminmax':\n",
    "                xw = bboxs[:, (1, 3)] * img_shape[1]\n",
    "                yh = bboxs[:, (2, 4)] * img_shape[0]\n",
    "                xmin = xw[:, 0] - xw[:, 1] / 2\n",
    "                xmax = xw[:, 0] + xw[:, 1] / 2\n",
    "                ymin = yh[:, 0] - yh[:, 1] / 2\n",
    "                ymax = yh[:, 0] + yh[:, 1] / 2\n",
    "                coords_converted = np.column_stack((xmin, ymin, xmax, ymax))\n",
    "\n",
    "        return coords_converted\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images and boxes\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs[idx])\n",
    "        annot_path = os.path.join(self.root, \"labels\", self.annots[idx])\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        img= img/255.0\n",
    "\n",
    "        # retrieve bbox list and format to required type,\n",
    "        # if annotation file is empty, fill dummy box with label 0\n",
    "        if os.path.getsize(annot_path) != 0:\n",
    "            bboxs = np.loadtxt(annot_path, ndmin=2)\n",
    "            bboxs = self.convert_box_cord(bboxs, 'normxywh', 'xyminmax', img.shape)\n",
    "            num_objs = len(bboxs)\n",
    "            bboxs = torch.as_tensor(bboxs, dtype=torch.float32)\n",
    "            # there is only one class\n",
    "            labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "            # suppose all instances are not crowd\n",
    "            iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        else:\n",
    "            bboxs = torch.as_tensor([[0, 0, 640, 640]], dtype=torch.float32)\n",
    "            labels = torch.zeros((1,), dtype=torch.int64)\n",
    "            iscrowd = torch.zeros((1,), dtype=torch.int64)\n",
    "\n",
    "        area = (bboxs[:, 3] - bboxs[:, 1]) * (bboxs[:, 2] - bboxs[:, 0])\n",
    "        image_id = torch.tensor([idx])\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = bboxs\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            sample = self.transforms(image=img,\n",
    "                                     bboxes=target['boxes'],\n",
    "                                     labels=labels)\n",
    "        img = sample['image']\n",
    "        target['boxes'] = torch.tensor(sample['bboxes'])\n",
    "        target['labels'] = torch.tensor(sample['labels'])\n",
    "        if target['boxes'].ndim == 1:\n",
    "            target['boxes'] = torch.as_tensor([[0, 0, 640, 640]], dtype=torch.float32)\n",
    "            target['labels'] = torch.zeros((1,), dtype=torch.int64)\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_bbox(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    if train:\n",
    "        return A.Compose([\n",
    "            # A.Flip(p=0.5),\n",
    "            # A.RandomResizedCrop(height=640,width=640,p=0.4),\n",
    "            # # A.Perspective(p=0.4),\n",
    "            # A.Rotate(p=0.5),\n",
    "            # # A.Transpose(p=0.3),\n",
    "            ToTensorV2(p=1.0)],\n",
    "            bbox_params=A.BboxParams(format='pascal_voc',min_visibility=0.4, label_fields=['labels']))\n",
    "    else:\n",
    "        return A.Compose([ToTensorV2(p=1.0)],\n",
    "                         bbox_params=A.BboxParams(format='pascal_voc', min_visibility=0.5, label_fields=['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "  '''\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "  '''\n",
    "  for layer in m.children():\n",
    "    if hasattr(layer, 'reset_parameters'):\n",
    "        print(f'Reset trainable parameters of layer = {layer}')\n",
    "        layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images with annotations have been saved to D:\\Prayag Files\\TIET\\Extras\\Internship\\Ongoing\\hexagon\\craters\\output_images.\n",
      "All labels have been saved to D:\\Prayag Files\\TIET\\Extras\\Internship\\Ongoing\\hexagon\\craters\\labels.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "class CraterBoulderDataset:\n",
    "    def __init__(self, images_dir, labels_dir):\n",
    "        self.images_dir = images_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        self.image_paths = sorted(glob.glob(os.path.join(images_dir, '*.jpg')))  # Adjust file extension as per your images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label_path = os.path.join(self.labels_dir, os.path.splitext(os.path.basename(img_path))[0] + '.txt')\n",
    "\n",
    "        # Load image\n",
    "        img = plt.imread(img_path)\n",
    "\n",
    "        # Load labels\n",
    "        labels = []\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) == 5:\n",
    "                        class_label = int(parts[0])  # Assuming class label is present\n",
    "                        x_center = float(parts[1]) * img.shape[1]  # Convert to absolute coordinates\n",
    "                        y_center = float(parts[2]) * img.shape[0]\n",
    "                        width = float(parts[3]) * img.shape[1]\n",
    "                        height = float(parts[4]) * img.shape[0]\n",
    "                        labels.append({\n",
    "                            'class_label': class_label,\n",
    "                            'x_center': x_center,\n",
    "                            'y_center': y_center,\n",
    "                            'width': width,\n",
    "                            'height': height\n",
    "                        })\n",
    "\n",
    "        return img, labels\n",
    "\n",
    "# Function to plot image with bounding boxes and save to file\n",
    "def plot_img_bbox_save(img, labels, save_path):\n",
    "    fig, a = plt.subplots(1, 1)\n",
    "    fig.set_size_inches(8, 8)\n",
    "    a.imshow(img)\n",
    "\n",
    "    for label in labels:\n",
    "        x_center = label['x_center']\n",
    "        y_center = label['y_center']\n",
    "        width = label['width']\n",
    "        height = label['height']\n",
    "\n",
    "        x_min = x_center - width / 2\n",
    "        y_min = y_center - height / 2\n",
    "        x_max = x_center + width / 2\n",
    "        y_max = y_center + height / 2\n",
    "\n",
    "        rect = patches.Rectangle((x_min, y_min), (x_max - x_min), (y_max - y_min),\n",
    "                                 edgecolor='r', facecolor='none', clip_on=False)  # Changed color to red\n",
    "        a.add_patch(rect)\n",
    "        a.annotate('Crater', (x_min, y_min - 10), color='red', weight='bold',  # Changed color to red\n",
    "                   fontsize=8, ha='left', va='top')\n",
    "\n",
    "    plt.axis('off')  # Hide axes\n",
    "    plt.savefig(save_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "# Function to save labels and crater count to CSV\n",
    "# Function to save labels to CSV with the number of craters detected in each image\n",
    "def save_labels_to_csv(dataset, csv_path):\n",
    "    rows = []\n",
    "    for idx in range(len(dataset)):\n",
    "        img_path = dataset.image_paths[idx]\n",
    "        labels = dataset[idx][1]  # Get labels for the image\n",
    "        num_craters = len(labels)  # Number of craters in the image\n",
    "\n",
    "        for label in labels:\n",
    "            rows.append({\n",
    "                'image_path': os.path.basename(img_path),\n",
    "                'class_label': label['class_label'],\n",
    "                'x_center': label['x_center'],\n",
    "                'y_center': label['y_center'],\n",
    "                'width': label['width'],\n",
    "                'height': label['height'],\n",
    "                'num_craters': num_craters\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "# Example usage\n",
    "images_dir = r'D:\\Prayag Files\\TIET\\Extras\\Internship\\Ongoing\\hexagon\\craters\\train\\images'\n",
    "labels_dir = r'D:\\Prayag Files\\TIET\\Extras\\Internship\\Ongoing\\hexagon\\craters\\train\\labels'\n",
    "output_dir = r'D:\\Prayag Files\\TIET\\Extras\\Internship\\Ongoing\\hexagon\\craters\\output_images'\n",
    "csv_file = r'D:\\Prayag Files\\TIET\\Extras\\Internship\\Ongoing\\hexagon\\craters\\labels.csv'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "dataset = CraterBoulderDataset(images_dir, labels_dir)\n",
    "\n",
    "# Plot and save all images with annotations\n",
    "for idx in range(len(dataset)):\n",
    "    img, labels = dataset[idx]\n",
    "    file_name = os.path.basename(dataset.image_paths[idx])\n",
    "    save_path = os.path.join(output_dir, file_name)\n",
    "    plot_img_bbox_save(img, labels, save_path)\n",
    "\n",
    "# Save labels to CSV\n",
    "save_labels_to_csv(dataset, csv_file)\n",
    "\n",
    "print(f\"All images with annotations have been saved to {output_dir}.\")\n",
    "print(f\"All labels have been saved to {csv_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prayag Chawla\\AppData\\Local\\Temp\\ipykernel_30844\\875038580.py:75: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import random\n",
    "\n",
    "class CraterBoulderDataset:\n",
    "    def __init__(self, images_dir, labels_dir):\n",
    "        self.images_dir = images_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        self.image_paths = sorted(glob.glob(os.path.join(images_dir, '*.jpg')))  # Adjust file extension as per your images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label_path = os.path.join(self.labels_dir, os.path.splitext(os.path.basename(img_path))[0] + '.txt')\n",
    "\n",
    "        # Load image\n",
    "        img = plt.imread(img_path)\n",
    "\n",
    "        # Load labels\n",
    "        labels = []\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) == 5:\n",
    "                        class_label = int(parts[0])  # Assuming class label is present\n",
    "                        x_center = float(parts[1]) * img.shape[1]  # Convert to absolute coordinates\n",
    "                        y_center = float(parts[2]) * img.shape[0]\n",
    "                        width = float(parts[3]) * img.shape[1]\n",
    "                        height = float(parts[4]) * img.shape[0]\n",
    "                        labels.append({\n",
    "                            'class_label': class_label,\n",
    "                            'x_center': x_center,\n",
    "                            'y_center': y_center,\n",
    "                            'width': width,\n",
    "                            'height': height\n",
    "                        })\n",
    "\n",
    "        return img, labels\n",
    "\n",
    "# Example usage\n",
    "images_dir = r'D:\\Prayag Files\\TIET\\Extras\\Internship\\Ongoing\\hexagon\\craters\\train\\images'\n",
    "labels_dir = r'D:\\Prayag Files\\TIET\\Extras\\Internship\\Ongoing\\hexagon\\craters\\train\\labels'\n",
    "\n",
    "dataset = CraterBoulderDataset(images_dir, labels_dir)\n",
    "\n",
    "# Plotting function with 'Crater' annotation and red color\n",
    "def plot_img_bbox(img, labels):\n",
    "    fig, a = plt.subplots(1, 1)\n",
    "    fig.set_size_inches(8, 8)\n",
    "    a.imshow(img)\n",
    "\n",
    "    for label in labels:\n",
    "        x_center = label['x_center']\n",
    "        y_center = label['y_center']\n",
    "        width = label['width']\n",
    "        height = label['height']\n",
    "\n",
    "        x_min = x_center - width / 2\n",
    "        y_min = y_center - height / 2\n",
    "        x_max = x_center + width / 2\n",
    "        y_max = y_center + height / 2\n",
    "\n",
    "        rect = patches.Rectangle((x_min, y_min), (x_max - x_min), (y_max - y_min),\n",
    "                                 edgecolor='r', facecolor='none', clip_on=False)  # Changed color to red\n",
    "        a.add_patch(rect)\n",
    "        a.annotate('Crater', (x_min, y_min - 10), color='red', weight='bold',  # Changed color to red\n",
    "                   fontsize=8, ha='left', va='top')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Print 3 random examples to check annotations with red color\n",
    "for idx in random.sample(range(len(dataset)), 3):\n",
    "    img, labels = dataset[idx]\n",
    "    plot_img_bbox(img, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class CraterBoulderDataset(Dataset):\n",
    "    def __init__(self, images_dir, labels_dir):\n",
    "        self.images_dir = images_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        self.image_paths = sorted(glob.glob(os.path.join(images_dir, '*.jpg')))  # Adjust file extension as needed\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label_path = os.path.join(self.labels_dir, os.path.splitext(os.path.basename(img_path))[0] + '.txt')\n",
    "\n",
    "        # Load image\n",
    "        img = plt.imread(img_path)\n",
    "        img = torch.tensor(img).permute(2, 0, 1).float()  # Convert to PyTorch tensor and reorder dimensions\n",
    "\n",
    "        # Load labels\n",
    "        labels = []\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) == 5:\n",
    "                        class_label = int(parts[0])  # Assuming class label is present\n",
    "                        x_center = float(parts[1]) * img.shape[2]  # Convert to absolute coordinates\n",
    "                        y_center = float(parts[2]) * img.shape[1]\n",
    "                        width = float(parts[3]) * img.shape[2]\n",
    "                        height = float(parts[4]) * img.shape[1]\n",
    "                        labels.append({\n",
    "                            'class_label': class_label,\n",
    "                            'x_center': x_center,\n",
    "                            'y_center': y_center,\n",
    "                            'width': width,\n",
    "                            'height': height\n",
    "                        })\n",
    "\n",
    "        # Convert to PyTorch format\n",
    "        target = {\n",
    "            'boxes': torch.tensor([\n",
    "                [label['x_center'] - label['width'] / 2, \n",
    "                 label['y_center'] - label['height'] / 2, \n",
    "                 label['x_center'] + label['width'] / 2, \n",
    "                 label['y_center'] + label['height'] / 2]\n",
    "                for label in labels\n",
    "            ], dtype=torch.float32),\n",
    "            'labels': torch.tensor([label['class_label'] for label in labels], dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "        return img, target\n",
    "\n",
    "# Create the dataset and DataLoader\n",
    "dataset = CraterBoulderDataset(images_dir, labels_dir)\n",
    "data_loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=lambda x: x)  # Adjust batch_size as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def convert_labels(images_dir, labels_dir):\n",
    "    # Get all image paths\n",
    "    image_paths = glob.glob(os.path.join(images_dir, '*.jpg'))  # Adjust file extension if needed\n",
    "    \n",
    "    for img_path in image_paths:\n",
    "        # Corresponding label file\n",
    "        label_path = os.path.join(labels_dir, os.path.splitext(os.path.basename(img_path))[0] + '.txt')\n",
    "        \n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "                \n",
    "            # Convert each line\n",
    "            with open(label_path, 'w') as file:\n",
    "                for line in lines:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) == 5:\n",
    "                        class_id, x_center, y_center, width, height = parts\n",
    "                        file.write(f\"{class_id} {x_center} {y_center} {width} {height}\\n\")\n",
    "                    else:\n",
    "                        print(f\"Invalid format in {label_path}: {line}\")\n",
    "\n",
    "# Define paths\n",
    "images_dir = 'D:/Prayag Files/TIET/Extras/Internship/Ongoing/hexagon/craters/train/images'\n",
    "labels_dir = 'D:/Prayag Files/TIET/Extras/Internship/Ongoing/hexagon/craters/train/labels'\n",
    "\n",
    "# Convert label files\n",
    "convert_labels(images_dir, labels_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRO TIP  Replace 'model=yolov5s.pt' with new 'model=yolov5su.pt'.\n",
      "YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.61 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.2.60  Python-3.11.0 torch-2.0.1+cpu CPU (11th Gen Intel Core(TM) i5-1135G7 2.40GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov5s.pt, data=data.yaml, epochs=10, time=None, patience=100, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=runs_train, name=exp, exist_ok=True, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs_train\\exp\n",
      "Overriding model.yaml nc=80 with nc=2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      3520  ultralytics.nn.modules.conv.Conv             [3, 32, 6, 2, 2]              \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     18816  ultralytics.nn.modules.block.C3              [64, 64, 1]                   \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    115712  ultralytics.nn.modules.block.C3              [128, 128, 2]                 \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  3    625152  ultralytics.nn.modules.block.C3              [256, 256, 3]                 \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1182720  ultralytics.nn.modules.block.C3              [512, 512, 1]                 \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1    131584  ultralytics.nn.modules.conv.Conv             [512, 256, 1, 1]              \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    361984  ultralytics.nn.modules.block.C3              [512, 256, 1, False]          \n",
      " 14                  -1  1     33024  ultralytics.nn.modules.conv.Conv             [256, 128, 1, 1]              \n",
      " 15                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 16             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 17                  -1  1     90880  ultralytics.nn.modules.block.C3              [256, 128, 1, False]          \n",
      " 18                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 19            [-1, 14]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 20                  -1  1    296448  ultralytics.nn.modules.block.C3              [256, 256, 1, False]          \n",
      " 21                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 22            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 23                  -1  1   1182720  ultralytics.nn.modules.block.C3              [512, 512, 1, False]          \n",
      " 24        [17, 20, 23]  1   2116822  ultralytics.nn.modules.head.Detect           [2, [128, 256, 512]]          \n",
      "YOLOv5s summary: 262 layers, 9,122,966 parameters, 9,122,950 gradients, 24.0 GFLOPs\n",
      "\n",
      "Transferred 421/427 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs_train\\exp', view at http://localhost:6006/\n",
      "Freezing layer 'model.24.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\Prayag Files\\TIET\\Extras\\Internship\\Ongoing\\hexagon\\craters\\train\\labels.cache... 98 images, 9 backgrounds, 0 corrupt: 100%|██████████| 98/98 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\Prayag Files\\TIET\\Extras\\Internship\\Ongoing\\hexagon\\craters\\train\\labels.cache... 98 images, 9 backgrounds, 0 corrupt: 100%|██████████| 98/98 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs_train\\exp\\labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No such keys(s): 'mode.use_inf_as_null'\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 69 weight(decay=0.0), 76 weight(decay=0.0005), 75 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns_train\\exp\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10         0G      1.953      3.038      1.515          9        640: 100%|██████████| 25/25 [02:41<00:00,  6.44s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [01:14<00:00,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         98        681      0.596      0.502      0.497      0.266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10         0G      1.788      2.935      1.447         28        640: 100%|██████████| 25/25 [02:43<00:00,  6.53s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [01:09<00:00,  5.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         98        681      0.214      0.258      0.205      0.118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10         0G      1.667      2.345      1.326         48        640: 100%|██████████| 25/25 [02:32<00:00,  6.12s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [01:05<00:00,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         98        681      0.424      0.536      0.383      0.192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10         0G      1.755      2.775      1.452         16        640: 100%|██████████| 25/25 [02:47<00:00,  6.70s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [01:04<00:00,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         98        681      0.501      0.496      0.465      0.225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10         0G      1.688      2.705      1.411          0        640: 100%|██████████| 25/25 [02:12<00:00,  5.30s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:51<00:00,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         98        681      0.588      0.476      0.537      0.276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10         0G      1.697      2.186      1.384         28        640: 100%|██████████| 25/25 [02:46<00:00,  6.67s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [01:05<00:00,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         98        681       0.51      0.489      0.486      0.262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10         0G      1.683       2.12      1.429          2        640: 100%|██████████| 25/25 [02:43<00:00,  6.55s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:55<00:00,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         98        681       0.58      0.551       0.57      0.308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10         0G      1.648      1.774      1.403          5        640: 100%|██████████| 25/25 [02:24<00:00,  5.80s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:56<00:00,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         98        681      0.629      0.553      0.611      0.354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10         0G      1.598      1.665      1.351         30        640: 100%|██████████| 25/25 [02:42<00:00,  6.49s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:57<00:00,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         98        681       0.68      0.564       0.65      0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10         0G      1.589      1.621      1.312         15        640: 100%|██████████| 25/25 [02:54<00:00,  6.97s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [01:01<00:00,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         98        681      0.699      0.573      0.663       0.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 epochs completed in 0.623 hours.\n",
      "Optimizer stripped from runs_train\\exp\\weights\\last.pt, 18.5MB\n",
      "Optimizer stripped from runs_train\\exp\\weights\\best.pt, 18.5MB\n",
      "\n",
      "Validating runs_train\\exp\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.2.60  Python-3.11.0 torch-2.0.1+cpu CPU (11th Gen Intel Core(TM) i5-1135G7 2.40GHz)\n",
      "YOLOv5s summary (fused): 193 layers, 9,112,310 parameters, 0 gradients, 23.8 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:56<00:00,  4.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         98        681        0.7      0.573      0.665      0.391\n",
      "                crater         89        681        0.7      0.573      0.665      0.391\n",
      "Speed: 3.1ms preprocess, 537.3ms inference, 0.0ms loss, 11.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns_train\\exp\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>▃▅▇█▇▆▅▃▂▁</td></tr><tr><td>lr/pg1</td><td>▃▅▇█▇▆▅▃▂▁</td></tr><tr><td>lr/pg2</td><td>▃▅▇█▇▆▅▃▂▁</td></tr><tr><td>metrics/mAP50(B)</td><td>▅▁▄▅▆▅▇▇██</td></tr><tr><td>metrics/mAP50-95(B)</td><td>▅▁▃▄▅▅▆▇██</td></tr><tr><td>metrics/precision(B)</td><td>▆▁▄▅▆▅▆▇██</td></tr><tr><td>metrics/recall(B)</td><td>▆▁▇▆▆▆▇███</td></tr><tr><td>model/GFLOPs</td><td>▁</td></tr><tr><td>model/parameters</td><td>▁</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>▁</td></tr><tr><td>train/box_loss</td><td>█▅▃▄▃▃▃▂▁▁</td></tr><tr><td>train/cls_loss</td><td>█▇▅▇▆▄▃▂▁▁</td></tr><tr><td>train/dfl_loss</td><td>█▆▁▆▄▃▅▄▂▁</td></tr><tr><td>val/box_loss</td><td>▇███▇▅▅▃▁▁</td></tr><tr><td>val/cls_loss</td><td>▅███▃▄▃▂▁▁</td></tr><tr><td>val/dfl_loss</td><td>▄▄▅██▃▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>0.00018</td></tr><tr><td>lr/pg1</td><td>0.00018</td></tr><tr><td>lr/pg2</td><td>0.00018</td></tr><tr><td>metrics/mAP50(B)</td><td>0.66498</td></tr><tr><td>metrics/mAP50-95(B)</td><td>0.39054</td></tr><tr><td>metrics/precision(B)</td><td>0.70039</td></tr><tr><td>metrics/recall(B)</td><td>0.57326</td></tr><tr><td>model/GFLOPs</td><td>24.045</td></tr><tr><td>model/parameters</td><td>9122966</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>601.776</td></tr><tr><td>train/box_loss</td><td>1.58868</td></tr><tr><td>train/cls_loss</td><td>1.62149</td></tr><tr><td>train/dfl_loss</td><td>1.31192</td></tr><tr><td>val/box_loss</td><td>1.58226</td></tr><tr><td>val/cls_loss</td><td>1.67537</td></tr><tr><td>val/dfl_loss</td><td>1.3287</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">exp</strong> at: <a href='https://wandb.ai/chawlapc-619-thapar-university/runs_train/runs/tpn8oluy' target=\"_blank\">https://wandb.ai/chawlapc-619-thapar-university/runs_train/runs/tpn8oluy</a><br/>Synced 5 W&B file(s), 20 media file(s), 5 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240721_030245-tpn8oluy\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to runs_train/exp/weights/best.pt\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Define paths\n",
    "train_images_dir = 'D:/Prayag Files/TIET/Extras/Internship/Ongoing/hexagon/craters/train/images'\n",
    "train_labels_dir = 'D:/Prayag Files/TIET/Extras/Internship/Ongoing/hexagon/craters/train/labels'\n",
    "val_images_dir = 'D:/Prayag Files/TIET/Extras/Internship/Ongoing/hexagon/craters/train/images'  # Reuse train images\n",
    "val_labels_dir = 'D:/Prayag Files/TIET/Extras/Internship/Ongoing/hexagon/craters/train/labels'  # Reuse train labels\n",
    "data_cfg_path = 'data.yaml'\n",
    "\n",
    "# Create YOLOv5 dataset YAML configuration\n",
    "data_cfg = {\n",
    "    'train': train_images_dir,\n",
    "    'val': val_images_dir,\n",
    "    'nc': 2,  # Number of classes\n",
    "    'names': ['crater', 'boulder']  # Class names\n",
    "}\n",
    "\n",
    "# Save data configuration to a YAML file\n",
    "with open(data_cfg_path, 'w') as f:\n",
    "    yaml.dump(data_cfg, f)\n",
    "\n",
    "# Load YOLOv5 model\n",
    "model = YOLO('yolov5s.pt')  # Load a pre-trained YOLOv5 model\n",
    "\n",
    "# Train the model\n",
    "model.train(\n",
    "    data=data_cfg_path,\n",
    "    epochs=10,  # Number of epochs\n",
    "    imgsz=640,  # Image size\n",
    "    batch=4,    # Batch size\n",
    "    project='runs_train',  # Directory for saving results\n",
    "    name='exp',  # Experiment name\n",
    "    exist_ok=True  # Overwrite existing runs\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model_path = 'runs_train/exp/weights/best.pt'\n",
    "model.save(model_path)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
